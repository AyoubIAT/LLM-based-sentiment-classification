{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"data_with_embeddings.pkl\")\n",
    "test_df = pd.read_pickle(\"test_data_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27480\n",
      "768\n",
      "3534\n"
     ]
    }
   ],
   "source": [
    "X_train = list(train_df[\"embedding\"])\n",
    "y_train = train_df[\"sentiment_class\"]\n",
    "X_test = list(test_df[\"embedding\"])\n",
    "y_test = test_df[\"sentiment_class\"]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_train[0]))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_train)\n",
    "y_test = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  \n",
    "        self.fc2 = nn.Linear(512, 256)         \n",
    "        self.fc3 = nn.Linear(256, 128)         \n",
    "        self.fc4 = nn.Linear(128, 64)          \n",
    "        self.fc5 = nn.Linear(64, 3)   \n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.fc1(x))\n",
    "        x = self.gelu(self.fc2(x))\n",
    "        x = self.gelu(self.fc3(x))\n",
    "        x = self.gelu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_dim=len(X_train[0])).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3641216456890106\n",
      "Epoch 2, Loss: 0.4677952527999878\n",
      "Epoch 3, Loss: 0.5310391187667847\n",
      "Epoch 4, Loss: 0.4388793706893921\n",
      "Epoch 5, Loss: 0.41193705797195435\n",
      "Epoch 6, Loss: 0.32082995772361755\n",
      "Epoch 7, Loss: 0.30325859785079956\n",
      "Epoch 8, Loss: 0.30541402101516724\n",
      "Epoch 9, Loss: 0.2734943628311157\n",
      "Epoch 10, Loss: 0.24180345237255096\n",
      "Epoch 11, Loss: 0.37968939542770386\n",
      "Epoch 12, Loss: 0.4422644376754761\n",
      "Epoch 13, Loss: 0.20456074178218842\n",
      "Epoch 14, Loss: 0.21106351912021637\n",
      "Epoch 15, Loss: 0.05848437920212746\n",
      "Epoch 16, Loss: 0.06673743575811386\n",
      "Epoch 17, Loss: 0.14521242678165436\n",
      "Epoch 18, Loss: 0.08019820600748062\n",
      "Epoch 19, Loss: 0.0543985478579998\n",
      "Epoch 20, Loss: 0.0698603019118309\n",
      "Epoch 21, Loss: 0.03990492969751358\n",
      "Epoch 22, Loss: 0.021909642964601517\n",
      "Epoch 23, Loss: 0.036552026867866516\n",
      "Epoch 24, Loss: 0.07765203714370728\n",
      "Epoch 25, Loss: 0.04688672721385956\n",
      "Epoch 26, Loss: 0.07496756315231323\n",
      "Epoch 27, Loss: 0.0705350935459137\n",
      "Epoch 28, Loss: 0.03933535888791084\n",
      "Epoch 29, Loss: 0.053486380726099014\n",
      "Epoch 30, Loss: 0.10212487727403641\n",
      "Epoch 31, Loss: 0.05119001492857933\n",
      "Epoch 32, Loss: 0.028804995119571686\n",
      "Epoch 33, Loss: 0.01411584671586752\n",
      "Epoch 34, Loss: 0.12027807533740997\n",
      "Epoch 35, Loss: 0.05092698335647583\n",
      "Epoch 36, Loss: 0.0689656063914299\n",
      "Epoch 37, Loss: 0.06476784497499466\n",
      "Epoch 38, Loss: 0.050912726670503616\n",
      "Epoch 39, Loss: 0.024285942316055298\n",
      "Epoch 40, Loss: 0.07170131802558899\n",
      "Epoch 41, Loss: 0.0331982783973217\n",
      "Epoch 42, Loss: 0.008389434777200222\n",
      "Epoch 43, Loss: 0.12156885117292404\n",
      "Epoch 44, Loss: 0.053953900933265686\n",
      "Epoch 45, Loss: 0.04457727447152138\n",
      "Epoch 46, Loss: 0.024722926318645477\n",
      "Epoch 47, Loss: 0.026576517149806023\n",
      "Epoch 48, Loss: 0.10179198533296585\n",
      "Epoch 49, Loss: 0.013891094364225864\n",
      "Epoch 50, Loss: 0.03213924169540405\n",
      "Epoch 51, Loss: 0.04152917489409447\n",
      "Epoch 52, Loss: 0.07052933424711227\n",
      "Epoch 53, Loss: 0.0649576261639595\n",
      "Epoch 54, Loss: 0.017510272562503815\n",
      "Epoch 55, Loss: 0.019592544063925743\n",
      "Epoch 56, Loss: 0.03173086419701576\n",
      "Epoch 57, Loss: 0.03929024189710617\n",
      "Epoch 58, Loss: 0.044607166200876236\n",
      "Epoch 59, Loss: 0.026858888566493988\n",
      "Epoch 60, Loss: 0.07323502749204636\n",
      "Epoch 61, Loss: 0.08785953372716904\n",
      "Epoch 62, Loss: 0.07395035028457642\n",
      "Epoch 63, Loss: 0.030315471813082695\n",
      "Epoch 64, Loss: 0.037088990211486816\n",
      "Epoch 65, Loss: 0.07192213088274002\n",
      "Epoch 66, Loss: 0.012885822914540768\n",
      "Epoch 67, Loss: 0.008137977682054043\n",
      "Epoch 68, Loss: 0.06962107121944427\n",
      "Epoch 69, Loss: 0.0961465910077095\n",
      "Epoch 70, Loss: 0.020066089928150177\n",
      "Epoch 71, Loss: 0.05212051793932915\n",
      "Epoch 72, Loss: 0.1079888865351677\n",
      "Epoch 73, Loss: 0.07136000692844391\n",
      "Epoch 74, Loss: 0.06459951400756836\n",
      "Epoch 75, Loss: 0.07987996935844421\n",
      "Epoch 76, Loss: 0.011185607872903347\n",
      "Epoch 77, Loss: 0.05541577190160751\n",
      "Epoch 78, Loss: 0.09723367542028427\n",
      "Epoch 79, Loss: 0.08515061438083649\n",
      "Epoch 80, Loss: 0.032665472477674484\n",
      "Epoch 81, Loss: 0.03974844142794609\n",
      "Epoch 82, Loss: 0.076450876891613\n",
      "Epoch 83, Loss: 0.04121188074350357\n",
      "Epoch 84, Loss: 0.037875860929489136\n",
      "Epoch 85, Loss: 0.055380817502737045\n",
      "Epoch 86, Loss: 0.04843621328473091\n",
      "Epoch 87, Loss: 0.056082893162965775\n",
      "Epoch 88, Loss: 0.027010111138224602\n",
      "Epoch 89, Loss: 0.011705723591148853\n",
      "Epoch 90, Loss: 0.017168192192912102\n",
      "Epoch 91, Loss: 0.026368416845798492\n",
      "Epoch 92, Loss: 0.07585173845291138\n",
      "Epoch 93, Loss: 0.09560158103704453\n",
      "Epoch 94, Loss: 0.057195983827114105\n",
      "Epoch 95, Loss: 0.020444724708795547\n",
      "Epoch 96, Loss: 0.06581559032201767\n",
      "Epoch 97, Loss: 0.03599618375301361\n",
      "Epoch 98, Loss: 0.03424146771430969\n",
      "Epoch 99, Loss: 0.08803514391183853\n",
      "Epoch 100, Loss: 0.032198891043663025\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_function(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.54%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure labels are in the correct format for multi-class classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)  # Use torch.long for multi-class labels\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "num_samples = len(X_test_tensor)  # Number of samples, not features\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    # Get the batch\n",
    "    X_batch = X_test_tensor[i:i+batch_size].to(device)\n",
    "    y_batch = y_test_tensor[i:i+batch_size].to(device)\n",
    "    \n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "    # Convert outputs to predicted class indices (class with the highest probability)\n",
    "    predicted = torch.argmax(outputs, dim=1)  # Get the class with the highest probability\n",
    "    \n",
    "    # Append true labels and predicted labels\n",
    "    y_true.extend(y_batch.cpu().numpy())  # Move to CPU if necessary\n",
    "    y_pred.extend(predicted.cpu().numpy())  # Move to CPU if necessary\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
